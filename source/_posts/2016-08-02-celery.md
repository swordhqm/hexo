---
title: celery
date: 2016-08-02 16:05:48
tags: 
    - python
    - django
    - 分布式任务队列
categories:
    - IT
---

# 安装celery
* 选择Broker
    RabbitMQ
    Redis
    database:
        SQLAlchemy
        Django Database
    others : Amazon SQS, Using MongoDB and IronMQ
    推荐RabbitMQ
    ```bash
    sudo dnf install rabbitmq-server
    systemctl start rabbitmq-server.service
    systemctl enable rabbitmq-server.service
    sudo rabbitmqctl add_user may_link password
    sudo rabbitmqctl add_vhost may_link
    sudo rabbitmqctl set_user_tags may_link may_link
    sudo rabbitmqctl set_permissions -p may_link may_link ".*" ".*" ".*"
    ```
    amazon rabbitmq 安装方法http://linuxsystemhacks.blogspot.com/2013/08/how-to-install-and-configure-rabbitmq.html

    celery 启动
    ```bash
    celery multi restart 1 -A FBA_Recall_Sys -l info -c4 --logfile=celery1.log --pidfile=celery1.pid
    ```

# 使用example

## 1.利用memcache 控制同一个task只会在同一时刻运行一个
```python
from __future__ import absolute_import

import os

from celery import Celery
from celery.task import task
from celery.utils.log import get_task_logger

# set the default Django settings module for the 'celery' program.
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'FBA_Recall_Sys.settings')

app = Celery('FBA_Recall_Sys')

# Using a string here means the worker will not have to
# pickle the object when using Windows.
app.config_from_object('django.conf:settings')
app.autodiscover_tasks(lambda: settings.INSTALLED_APPS)

logger = get_task_logger(__name__)

from FBA_Recall_Sys.models.replenish_plan import *
from FBA_Recall_Sys.utils import json_serialize,\
    last_visit,getSplitParagraph,trk_processing,\
    caculate_rate, generate_label_zip, draw_str_on_exsit,draw_on_exsit


import memcache
from django.conf import settings
from hashlib import md5

import time

LOCK_EXPIRE = 60 * 5 # Lock expires in 5 minutes

mem_cache = memcache.Client(['127.0.0.1:11211'])
    
@app.task(bind=True)
def debug_task(self):
    p1 = self.name
    p2 = 'A'
    lock_id = '{0}-lock-{1}'.format(p1, p2)
    
    cache = mem_cache
    # cache.add fails if the key already exists
    acquire_lock = lambda: cache.add('aa', 'true', LOCK_EXPIRE)
    # memcache delete is very slow, but we have to use it to take
    # advantage of using add() for atomic locking
    release_lock = lambda: cache.delete('aa')
    logger.info("########" + str(cache.get('aa')))
    if acquire_lock():
        logger.info("----------------------" + str(cache) + "----" + cache.get('aa'))
        try :
            import time
            for i in range(1, 10):
                time.sleep(1)
            logger.info("----" + cache.get("aa"))
            print('Request: {0!r}'.format(self.request))
        except Exception,e:
            logger.exception(e)
        finally:
            release_lock()
    
@app.task(bind=True)
def request_label_task(self, shipment_mark):
    logger.info("####task %s:%s" % (__file__, __name__))
    print('Request: {0!r}'.format(self.request))
    p1 = self.name
    p2 = shipment_mark
    lock_id = '{0}-lock-{1}'.format(p1, p2)
    cache = mem_cache
    # cache.add fails if the key already exists
    acquire_lock = lambda: cache.add(lock_id, 'true', LOCK_EXPIRE)
    # memcache delete is very slow, but we have to use it to take
    # advantage of using add() for atomic locking
    release_lock = lambda: cache.delete(lock_id)
    if acquire_lock():
        try:
            replenish_shipment_obj = Replenish_Shipment.select({'Shipment_mark':shipment_mark}, unique=True)
            if not replenish_shipment_obj:
                return
             
            #starting upload info & check if generating label
            _buf_object = replenish_shipment_obj.Replenish_Plan_ID
            pck_object = replenish_shipment_obj
            ps_object = pck_object
            if (not replenish_shipment_obj.Replenish_Plan_ID.IS_MANUAL_SUBMIT and
                not replenish_shipment_obj.label_path):
                mws_result = replenish_shipment_obj.get_label_for_2016_11_1(access_key=_buf_object.Amazon_account_ID.ACCESS_KEY,
                                secret_key=_buf_object.Amazon_account_ID.SECRET_KEY,
                                account_id=_buf_object.Amazon_account_ID.SELLER_ID,
                                marketplaceid=_buf_object.Amazon_account_ID.Marketplace_ID,
                                MWS_Auth_Token=_buf_object.Amazon_account_ID.MWS_Auth_Token,
                                )
                LABEL_FOLDER = settings.LABEL_FOLDER
                filename = generate_label_zip(mws_result['buf_pdfDocument'],LABEL_FOLDER + ps_object.Shipment_ID + '-' + str(time.time()).replace('.',''),'buf.zip')
                desname = filename.replace('PackageLabels','Print')
                draw_str_on_exsit(ps_object.Shipment_mark,filename,desname,x=500,y=-590,rotate=90,fontSize=35,auto_index=True)
                desname2 = filename.replace('PackageLabels','Print_ag')
                draw_str_on_exsit(ps_object.Shipment_mark,desname,desname2,x=100,y=-590,rotate=90,fontSize=35,auto_index=True)
                desname3 = filename.replace('PackageLabels','Print_ag2')
                draw_on_exsit(str(_buf_object.user_id.id),desname2,desname3,x1=100,y1=-510,x2=100,y2=-530,rotate=90,fontSize=50,auto_index=False)
                desname4 = filename.replace('PackageLabels','Print_ag3')
                draw_on_exsit(str(_buf_object.user_id.id),desname3,desname4,x1=500,y1=-510,x2=500,y2=-530,rotate=90,fontSize=50,auto_index=False)
                ps_object.update(label_path=desname4)
                ps_object.update(price=mws_result['est_price']['Value'] + ' ' + mws_result['est_price']['CurrencyCode'])
                ps_object.upload_tracking(mws_result['tracking_number'])
        except Exception,e:
            logger.exception(e)
        finally:
            release_lock()
```

```python
#settings.py 里修改添加amqp
BROKER_URL = 'amqp://may_link:password@localhost:5672/may_link'
```

## 2.Periodic Tasks（计划任务）
[文档](http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html)
需要考虑的东西：
    TimeZone
原理：
    通过add_periodic_task 将entry加入到app.conf.beat_schedule 配置。
启动：
    $ celery -A proj beat
    如果随着worker 启动也可以
    $ celery -A proj worker -B
    Beat needs to store the last run times of the tasks in a local database file (named celerybeat-schedule by default), so it needs access to write in the current directory, or alternatively you can specify a custom location for this file:
    $ celery -A proj beat -s /home/celery/var/run/celerybeat-schedule

# 文档learning

```python
#只有加入bind=True, self才可以指向当前Task instance。
@app.task(bind=True)
def debug_task(self):
    print('Request: {0!r}'.format(self.request)) #将self.request 用repr() format成串
```

```python
from celery import shared_task
#shared_task 表示可重用的task，没有上下文信息，只需要参数
@shared_task
def add(x, y):
    return x + y
```

停止worker
```shell
ps auxww | grep 'celery worker' | awk '{print $2}' | xargs kill -9
通常情况下，当前执行的task会丢失。但是如果task有设置acks_late选项，就不会看起来。
```

重启worker
```shell
celery multi start 1 -A proj -l info -c4 --pidfile=/var/run/celery/%n.pid
celery multi restart 1 --pidfile=/var/run/celery/%n.pid
或者
kill -HUP $pid

celery multi restart 1 -A FBA_Recall_Sys --logfile=./log/celery1.log --pidfile=./log/celery1.pid

[kevin@localhost freshingstock]$ ps -auxww | grep "celery"
kevin    29556  8.8  1.3 330708 56280 ?        S    10:08   0:00 [celeryd: celery1@localhost.localdomain:MainProcess] -active- (worker -A FBA_Recall_Sys --logfile=./log/celery1.log --pidfile=./log/celery1.pid --hostname=celery1@localhost.localdomain)
kevin    29567  0.0  1.1 330296 45924 ?        S    10:08   0:00 [celeryd: celery1@localhost.localdomain:Worker-1]
kevin    29568  0.0  1.1 330296 45912 ?        S    10:08   0:00 [celeryd: celery1@localhost.localdomain:Worker-2]
kevin    29569  0.0  1.1 330296 45928 ?        S    10:08   0:00 [celeryd: celery1@localhost.localdomain:Worker-3]
kevin    29570  0.0  1.1 330296 45840 ?        S    10:08   0:00 [celeryd: celery1@localhost.localdomain:Worker-4]
kevin    29574  0.0  0.0 118504  2388 pts/11   S+   10:08   0:00 grep --color=auto celery
[kevin@localhost freshingstock]$ ps --ppid 29556
  PID TTY          TIME CMD
29567 ?        00:00:00 [celeryd: celer
29568 ?        00:00:00 [celeryd: celer
29569 ?        00:00:00 [celeryd: celer
29570 ?        00:00:00 [celeryd: celer

可以看到一个主进程，下面4个子进程

celery multi restart 1 -A FBA_Recall_Sys -n worker.%h --logfile=./log/%n-%i.log --pidfile=./log/celery1.pid --autoreload
会把5个进程的log，单独定位到对应的log

标注 文档提到multiprocessing 和 eventlet。简单看来eventlet，看起来能够在IO 阻塞相关的多线程情形下有很大作用，类似一个CPU，多线程一致。
虽然 eventlet 封装成了非常类似标准线程库的形式，但线程和eventlet在实际并发执行流程仍然有明显区别。在没有出现 I/O 阻塞时，除非显式声明，否则当前正在执行的 eventlet 永远不会把 cpu 交给其他的 eventlet，而标准线程则是无论是否出现阻塞，总是由所有线程一起争夺运行资源。

```

Remote Control

```shell
就是可以观察Celery 组件的相关内容。
http://docs.celeryproject.org/en/latest/userguide/monitoring.html#monitoring-control
提到几个相关组件，Flower， Curses Monitor
```

Cocurrency

```shell
并发问题，目前可以看到实际应用中的情况，一个QueueServer， worker Server。假如worker Server在一台设备上。一种是celery官网上的CookBook 实例，另外一种是
启动多个celery node通过命令。
celery worker -A proj --cocurrency = 1 -Q Q1
celery worker -A proj --cocurrency = 4 -Q Q2
```

Exchanges, queues, and routing keys

```django
消息发到exchange，可能会是多个exchange么？
exchange 然后路由到多个queue
消息在queue里会被消费

```

# flower
安装 sudo pip install flower

如何使用:
celery multi restart 1 -A FBA_Recall_Sys --logfile=./log/celery1.log --pidfile=./log/celery1.pid
启动celery 进程池，会依连接到FBA_Recall_Sys下的settings.py里关于celery broker的配置。
然后通过
sudo rabbitmq-plugins enable rabbitmq_management
开启 Rabbitmq的API，这个会被flower调用到
然后 celery -A FBA_Recall_Sys flower
默认会开启一个local server，端口是5555
![flower_main](/img/flower_main.png "flower_main")

关于控制cocurrency
![flower_worker_control](/img/flower_worker_control.png "flower_worker_control")
